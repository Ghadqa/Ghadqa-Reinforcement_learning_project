{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BudgetAllocationEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(BudgetAllocationEnv, self).__init__()\n",
    "        \n",
    "        # Define action and observation space\n",
    "        # Actions: 0 = Increase, 1 = Maintain, 2 = Decrease\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        \n",
    "        # States: 0 = Low ROI, 1 = Moderate ROI, 2 = High ROI\n",
    "        self.observation_space = gym.spaces.Discrete(3)\n",
    "        \n",
    "        # Initialize state\n",
    "        self.state = 0\n",
    "        \n",
    "        # Rewards per state-action pair\n",
    "        self.reward_matrix = {\n",
    "        (0, 0): -10,   # Low ROI, Increase (bad choice)\n",
    "        (0, 1): -5,    # Low ROI, Maintain (neutral choice)\n",
    "        (0, 2): 10,    # Low ROI, Decrease (good choice)\n",
    "        (1, 0): 20,    # Moderate ROI, Increase (good choice)\n",
    "        (1, 1): 5,     # Moderate ROI, Maintain (neutral choice)\n",
    "        (1, 2): -5,    # Moderate ROI, Decrease (bad choice)\n",
    "        (2, 0): -5,    # High ROI, Increase (bad choice)\n",
    "        (2, 1): 15,    # High ROI, Maintain (good choice)\n",
    "        (2, 2): -10    # High ROI, Decrease (bad choice)\n",
    "}\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset to a random initial state (can be improved)\n",
    "        self.state = np.random.choice([0, 1, 2])\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Get reward based on current state and action\n",
    "        reward = self.reward_matrix.get((self.state, action), 0)\n",
    "        \n",
    "        # Transition logic: for simplicity, we'll keep it random\n",
    "        if action == 0 and self.state < 2:\n",
    "            next_state = self.state + 1\n",
    "        elif action == 2 and self.state > 0:\n",
    "            next_state = self.state - 1\n",
    "        else:\n",
    "            next_state = self.state\n",
    "        \n",
    "        # Check if the task is done (e.g., after 10 steps)\n",
    "        done = np.random.rand() > 0.95  # Arbitrary condition for episode end\n",
    "        \n",
    "        # Update state\n",
    "        self.state = next_state\n",
    "        \n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        state_names = ['Low ROI', 'Moderate ROI', 'High ROI']\n",
    "\n",
    "        print(f'Current State: {state_names[self.state]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RL_ Framework_solution \n",
    "def monte_carlo_learning(env, num_episodes=1000):\n",
    "    Q_table = np.zeros((2, 2, 3))  # Two channels, three actions\n",
    "    returns = { (s1, s2, a): [] for s1 in range(2) for s2 in range(2) for a in range(3) }  # Initialize returns\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = env.action_space.sample()  # Choose an action randomly\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        # Process the episode to update the Q-values\n",
    "        G = 0\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = reward + 0.9 * G  # Discounted reward\n",
    "            state_idx = state  # state_idx is directly the state in this simple case\n",
    "            returns[(state[0], state[1], action)].append(G)  # Store returns\n",
    "            Q_table[state[0], state[1], action] = np.mean(returns[(state[0], state[1], action)])  # Update Q-value\n",
    "\n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m BudgetAllocationEnv()\n\u001b[0;32m----> 2\u001b[0m Q_table \u001b[38;5;241m=\u001b[39m \u001b[43mmonte_carlo_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearned Q-table:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(Q_table)\n",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mmonte_carlo_learning\u001b[0;34m(env, num_episodes)\u001b[0m\n\u001b[1;32m     22\u001b[0m         G \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.9\u001b[39m \u001b[38;5;241m*\u001b[39m G  \u001b[38;5;66;03m# Discounted reward\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         state_idx \u001b[38;5;241m=\u001b[39m state  \u001b[38;5;66;03m# state_idx is directly the state in this simple case\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m         returns[(\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, state[\u001b[38;5;241m1\u001b[39m], action)]\u001b[38;5;241m.\u001b[39mappend(G)  \u001b[38;5;66;03m# Store returns\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         Q_table[state[\u001b[38;5;241m0\u001b[39m], state[\u001b[38;5;241m1\u001b[39m], action] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(returns[(state[\u001b[38;5;241m0\u001b[39m], state[\u001b[38;5;241m1\u001b[39m], action)])  \u001b[38;5;66;03m# Update Q-value\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Q_table\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "env = BudgetAllocationEnv()\n",
    "Q_table = monte_carlo_learning(env, num_episodes=1000)  # Train agent\n",
    "print(\"Learned Q-table:\")\n",
    "print(Q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
